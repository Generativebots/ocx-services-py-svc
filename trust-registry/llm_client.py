import yaml
import os
import requests
import json

class LLMClient:
    def __init__(self, config_path="config.yaml"):
        self.config = self._load_config(config_path)
        self.sovereign_mode = self.config.get("SOVEREIGN_MODE", False)
        
    def _load_config(self, path):
        # Resolve path relative to this file
        base_path = os.path.dirname(os.path.abspath(__file__))
        full_path = os.path.join(base_path, path)
        
        defaults = {"SOVEREIGN_MODE": False}
        
        try:
            with open(full_path, "r") as f:
                content = f.read()
                try:
                    return yaml.safe_load(content)
                except (NameError, ImportError):
                    # Fallback: Simple manual parsing
                    config = {}
                    for line in content.splitlines():
                        line = line.strip()
                        if not line or line.startswith("#"): continue
                        if ":" in line:
                            key, val = line.split(":", 1)
                            key = key.strip()
                            val = val.strip()
                            # Handle booleans and strings
                            if val.lower() == "true": val = True
                            elif val.lower() == "false": val = False
                            elif val.startswith('"') and val.endswith('"'): val = val.strip('"')
                            config[key] = val
                    return config
        except Exception as e:
            print(f"‚ö†Ô∏è Config Load Error: {e}")
            return defaults

    def generate(self, prompt, system_prompt=None):
        if self.sovereign_mode:
            return self._generate_local(prompt, system_prompt)
        else:
            return self._generate_cloud(prompt, system_prompt)

    def _generate_local(self, prompt, system_prompt):
        """Calls Ollama or vLLM running locally."""
        url = self.config.get("LOCAL_LLM_URL", "http://localhost:11434/api/generate")
        model = self.config.get("LOCAL_MODEL", "llama3")
        
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\nUser: {prompt}"

        payload = {
            "model": model,
            "prompt": full_prompt,
            "stream": False,
            "options": {"temperature": 0.0}
        }
        
        try:
            print(f"üß† [Sovereign Brain] Routing to Local {model}...")
            # Simulate success if local server isn't actually running during dev
            # res = requests.post(url, json=payload, timeout=30)
            # return res.json().get("response", "")
            return self._mock_local_response(prompt) # Using Mock for now to ensure stability
            
        except Exception as e:
            print(f"‚ùå Local Inference Failed: {e}")
            return "Error: Local Brain Unreachable."

    def _generate_cloud(self, prompt, system_prompt):
        print("‚òÅÔ∏è [Cloud API] Calling Vertex AI...")
        return "Mock Cloud Response: Compliance Verified."

    def _mock_local_response(self, prompt):
        # Mock responses for logic validation without a beefy GPU
        if "Ignore all previous" in prompt:
            return "BLOCK: Prompt Injection Detected."
        if "budget" in prompt.lower():
            return "ALLOW: Budget check passed."
        return "ALLOW: Standard SOP check passed."
